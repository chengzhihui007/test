下载资源：wget https://github.com/kubernetes/kubernetes/releases/download/v1.3.5/kubernetes.tar.gz

http://github.com/etcd-io/etcd/releases/download/v3.1.20/etcd-v3.1.20-linux-amd64.tar.gz


-----------------------------------------------------------
☆kubernetes.tar.gz内容简介
kubernetes.tar.gz包含了Kubernetes的服务程序文件、文档和示例。
解压后，server子目录中的kubernetes-server-linux-amd64.tar.gz文件包含了Kubernetes需要运行的全部服务程序文件。服务程序文件列表如图2.2所示:
	hyperkube ==> 总控程序，用于运行其他Kubernetes程序
	kube-apiserver ==> apiserver主程序
	kube-apiserver.docker_tag ==> apiserver docker 镜像的tag
	kube-apiserver.tar ==> apiserver docker 镜像文件
	kube-controller-manager ==> controller-manager主程序
	kube-controller-manager.docker_tag ==> controller-manager docker 镜像的tag
	kube-controller-manager.tar ==> controller-manager docker镜像文件
	kubectl ==> 客户端命令行工具
	kubelet ==> kubelet 主程序
	kube-proxy ==> proxy 主程序
	kube-scheduler ==> kube-scheduler 主程序
	kube-scheduler.docker_tag ==> kube-scheduler docker 镜像的tag
	kube-scheduler.tar ==> scheduler docker镜像文件
Kubernetes Master节点安装部署etcd、kube-apiserver、kube-controller-manager、kube-scheduler服务进程。我们使用kubectl作为客户端与Master进行交互操作，在工作Node上仅需部署kubelet和kube-proxy服务进程。Kubernetes还提供了一个“all-in-one”的hyperkube程序来完成对以上服务程序的启动。
--------------------------------------------------------------------
☆配置和启动Kubernetes服务(前置)
1.关闭防火墙
#systemctl disable firewalld 
#systemtl stop firewalld
2.将Kubernetes的可执行文件复制到/usr/bin(如果复制到其他目录，则将systemd服务文件中的文件路径修改正确即可)，然后对服务进行配置。
---------------------------------------
☆Master配置：etcd、kube-apiserver、kube-controller-manager、kube-scheduler服务
1)etcd服务
	参考网址：https://blog.csdn.net/god_wot/article/details/77854093(集群)

	etcd服务作为Kubernetes集群的主数据库，在安装Kubernetes各服务之前需要首先安装和启动。
	
	$ tar -zxvf  etcd-v3.2.6-linux-amd64.tar.gz -C /opt/
	$ cd /opt
	$ mv etcd-v3.2.6-linux-amd64  etcd-v3.2.6
	$ mkdir /etc/etcd           # 创建etcd配置文件目录


创建etcd配置文件：
	$ vi /etc/etcd/conf.yml
name: etcd-1
data-dir: /opt/etcd-v3.2.6/data
listen-client-urls: http://192.168.108.128:2379,http://127.0.0.1:2379
advertise-client-urls: http://192.168.108.128:2379,http://127.0.0.1:2379
listen-peer-urls: http://192.168.108.128:2380
initial-advertise-peer-urls: http://192.168.108.128:2380
initial-cluster: etcd-1=http://192.168.108.128:2380,etcd-2=http://192.168.108.129:2380,etcd-3=http://192.168.108.130:2380
initial-cluster-token: etcd-cluster-token
initial-cluster-state: new

当前使用的是etcd v3版本，系统默认的是v2，通过下面命令修改配置。

$ vi /etc/profile

在末尾追加

export ETCDCTL_API=3

$ source /etc/profile

编辑/usr/lib/systemd/system/etcd.service，添加下面内容：

[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/opt/etcd-v3.2.6/
# User=etcd
ExecStart=/opt/etcd-v3.2.6/etcd --config-file=/etc/etcd/conf.yml
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target


更新启动：
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
systemctl restart etcd

systemctl status etcd.service -l
2)kube-apiserver服务
编辑systemd服务文件/usr/lib/systemd/system/kube-apiserver.service,内容如下：
KUBE_API_ARGS="--etcd_servers=http://127.0.0.1:2379 --insecure-bind-address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range=169.169.0.0/16 --service-node-port-range=1-65535 --admission_control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota --logtostderr=false --log-dir=/var/log/kubernetes --v=2"

对启动参数的说明如下：
	○ --etcd_servers:指定etcd服务的url.
	○ --insecure-bind-address: apiserver 绑定主机的非安全IP地址，设置0.0.0.0表示绑定所有IP。
	○ --insecure-port: apiserver 绑定主机的非安全端口号，默认为8080.
	○ --service-cluster-ip-range:Kubernetes集群中Service的虚拟IP地址段范围，以CIDR格式表示，例如169.169.0.0/16，该IP范围不能与物理机的真实IP段有重合。
	○ --service-node-port-range:Kubernetes集群中Service可映射的物理机端口号范围，默认为30000~32767.	       ○ --admission_control: Kubernetes集群的准入控制设置，各控制模块以插件的形式一次生效。
	○ --logtostderr: 设置为false表示将日志写入文件，不写入stderr.
	○ --log-dir:日志目录。
	○ --v: 日志级别。

3)kube-controller-manager 服务
kube-controller-manager服务依赖于kube-apiserver服务。

#cat  /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=kube-apiserver.service
Requires=kube-apiserver.service

[Service]
EnvironmentFile=/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

配置文件/etc/kubernetes/controller-manager 的内容包括了kube-controller-manager的全部启动参数，主要的配置参数在变量KUBE_CONTROLLER_MANAGER_ARGS中指定。

#cat /etc/kubernetes/controller-manager
KUBE_CONTROLLER_MANAGER_ARGS="--master=http://192.168.134.134:8080 --logtostderr=false --log-dir=/var/log/kubernetes --v==2"

对启动参数的说明如下：
	○ --master:指定apiserver的URL地址。
	○ --logtostderr:设置为false表示将日志写入文件，不写入stderr.
	○ --log-dir: 日志目录
	○ --v: 日志级别。
4)kube-scheduler服务
kube-scheduler服务也依赖于kube-apiserver服务。
#cat /usr/lib/systemd/system/kube-scheduler.service

[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.co/GoogleCloudPlatform/kubernetes
After=kube-apiserver.service
Requires=kube-apiserver.service

[Service]
EnvironmentFile=/etc/kubernetes/scheduler
ExecStart=/usr/bin/kube-scheduler $KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

配置文件/etc/kubernetes/scheduler的内容包括了kube-scheduer的全部启动参数，主要的配资参数在变量KUBE_SCHEDULER_ARGS中指定。
#cat /etc/kubernetes/scheduler
KUBE_SCHEDULER_ARGS="--master=http://192.168.134.134:8080 --logtostderr=false --log-dir=/var/log/kubernetes --v=2"
对启动参数的说明如下：
	○ --master: 指定apiserver的URL地址。
	○ --logtostderr: 设置为false表示将日志写入文件，不写入stderr.
	○ --log-dir:日志目录。
	○ -v:日志级别。
配置完成后，执行systemctl start 命令按顺序启动这3个服务。同时，使用systemctl enable 命令将服务加入开机启动列表中。

	#systemctl daemon-reload
	#systemctl enable kube-apiserver.service
	#systemctl start kube-apiserver.service
	#systemctl enable kube-controller-manager
	#systemctl start kube-controller-manager
	#systemctl enable kube-scheduler
	#systemctl start kube-scheduler

通过systemctl status <service_name> 来验证服务的启动状态，"running"表示启动成功。
到此，Master上所需的服务就全部启动完成了。
-------------------------------------------------------------------------------------------------
☆Node上的kubelet、kube-proxy服务
在工作Node节点上需要预先安装好Docker Daemon 并且正常启动。Docker的安装详见http://www.docker.com的说明

参考网址：https://blog.csdn.net/qq_36892341/article/details/73918672

$uname -r 
大于3.10即可。
$yum update
$tee /etc/yum.repos.d/docker.repo<<-'EOF'
[dockerrepo]
name=Docker Respository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
EOF

$yum install -y docker-engine

$systemctl start docker.service

$docker version

$systemctl enable docker

1)kubelet服务
kubelet服务依赖于Docker服务。
#cat /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet $KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target

其中WorkingDirector表示kubelet保存数据的目录，需要在启动kubelet服务之前进行创建。
配置文件/etc/kubernetes/kubelet的内容包括了kubelet的全部启动参数，主要的配置参数在变量KUBELET_ARGS中指定。
#cat /etc/kubernetes/kubelet
KUBELET_ARGS="--api-servers=http://192.168.134.134:8080 --hostname-override=192.168.134.134 --logtostderr=false --log-dir=/var/log/kubernetes --v=2"

对启动参数的说明如下：
	○ --api-servers: 指定apiserver的URL地址，可以指定多个。
	○ --hostname-override: 设置本Node的名称。
	○ --logtostderr: 设置为false表示将日志写入文件，不写入stderr.
	○ --log-dir: 日志目录。
	○ --v: 日志级别。
2)kube-proxy服务
kube-proxy服务依赖于network服务。

[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
Requires=network.service

[Service]
WantedBy=multi-user.target

配置文件/etc/kubernetes/proxy的内容包括了kube-proxy的全部启动参数，主要的配置参数在变量KUBE_PROXY_ARGS中指定。
#cat /etc/kubernetes/proxy
KUBE_PROXY_ARGS="--master=http://192.168.134.134:8080 --logtostderr=false --log-dir=/var/log/kubernetes --v=2"
对启动参数的说明如下：
	○ --master:指定apiserver的URL地址。
	○ --logtostderr: 设置为false表示将日志写入文件，不写入stderr.
	○ --log-dir: 日志目录。
	○ --v: 日志级别。
配置完成后，通过systemctl 启动kubelet和 kube-proxy服务：
#systemctl deamon-reload
#systemctl enable kubelet.service
#systemctl start kubelet.service
#systemctl enable kube-proxy
#systemctl start kube-proxy

kubelet默认采用向Master自动注册本Node的机制，在Master上查看各Node的状态，状态为Ready表示Node已经成功注册并且状态为可用。
#kubectl get ndoes
NAME  STATUS  AGE
XX	Ready	1m

等所有Node的状态都为Ready之后，一个Kubernetes 集群就启动完成了。接下来就可以创建Pod、RC、Service等资源对象来部署Docker容器应用了。
-----------------------------------------------------------------
☆Kubernetes集群的安全设置
1.基于CA签名的双向数字证书认证方式
在一个安全的内网环境中，Kubernetes的各个组件与Master之间可以通过apiserver的非安全端口http://apiserver:8080进行访问。但如果apiserver需要对外提供服务，或者集群中的某些容器也需要访问apiserver以获取集群中的某些信息，则更安全的做法是启用HTTPS安全机制。Kubernetes提供了基于CA签名的双向数字证书认证方式和简单的基于HTTP BASE或TOKEN的认证方式，其中CA证书方式的安全性最高。本节先介绍以CA证书的方式配置Kubernetes集群，要求Master上的kube-apiserver、kube-controller-manager、kube-scheduler进程及各Node上的kubelet、kube-proxy进程进行CA签名双向数字证书安全设置。
基于CA签名的双向数字证书的生成过程如下：
(1)为kube-apiserver生成一个数字证书，并用CA证书进行签名。
(2)为kube-apiserver进程配置证书相关的启动参数，包括CA证书(用于验证客户端证书的签名真伪)、自己的经过CA签名后的证书及私钥。
(3)为每个访问Kubernetes API Server的客户端(如kube-controller-manager、kube-scheduler、kubelet、kube-proxy及调用API Server的客户端程序kubectl等)进程生成自己的数字证书，也都用CA证书进行签名，在相关程序的启动参数里增加CA证书、自己的证书等相关参数。

1) 设置kube-apiserver的CA证书相关的文件和启动参数
使用OpenSSL 工具在Master服务器上创建CA证书和私钥相关的文件：
	#openssl genrsa -out ca.key 2048
	#openssl req -x509 -new -nodes -key ca.key -subj "/CN=yourcompany.com" -days 5000 -out ca.crt
	#openssl genrsa -out server.key 2048
注意：生成ca.crt时，-subj参数中“/CN”的值通常为域名。
准备master-ssl.cnf文件，该文件用于x509 v3 版本的证书。在该文件中主要需要设置Master服务器的hostname(k8s-master)、IP地址(192.168.134.134),以及Kubernetes Master Service的虚拟服务名称(kubernetes.default等)和该虚拟服务的ClusterIP地址(16.169.0.1)。
master_ssl.cnf文件的示例如下：
[req]
req_extensions = v3_req
distinguished_name = req_distingished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @ alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
DNS.5 = k8s-master
IP.1 = 169.169.0.1
IP.2 = 192.168.134.134
-------------------------
☆Kubernetes的版本升级
	需要考虑到当前集群中正在运行的容器不受影响。应对集群中的各Node逐个进行隔离，然后等待在其上运行的容器全部执行完成，再更新该Node上的kubelete和kube-proxy服务，将全部Node都更新完成后，最后更新Master的服务。
	○通过官网下载最新的kubernetes.tar.gz，解压缩后提取服务二进制文件。
	○逐个隔离Node，等待在其上运行的全部容器工作完成，更新kubelet和kube-proxy服务文件，然后重启这2个服务。
	○更新Master的kube-apiserver、kube-controller-manager、kube-scheduler服务文件并重启。
---------------------------------------
